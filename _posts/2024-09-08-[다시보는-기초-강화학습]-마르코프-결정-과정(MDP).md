---
title: '[다시보는 기초 강화학습] 1. 마르코프 결정 과정(MDP), 가치함수'
date: 2024-09-08 14:20:00 +0900
categories: ['다시보는 기초 강화학습']
tags: [rl] # TAG names should always be lowercase
author: euijin_jeong
math: true
toc: true
---
> Note: 이 글은 책 Reinforcement Learning: an Introduction 을 보고 제 생각과 함께 정리한 내용입니다.

# 보상 가설, 그리고 강화학습
- 강화학습(Reinforcement Learning) : 보상을 최대화하는 정책을 찾기 위한 머신러닝 방법론.
- 보상 가설(Reward Hypothesis) : '어떤 형태의 목표도 보상(Reward)의 형태로 모델링 할 수 있다' 라는 가설.

즉, 강화학습은 보상 가설에 기반을 둔 머신러닝 방법론이다.

만약 강화학습이라는 개념이 없을 떄, 우리는 강화학습이라는 머신러닝 분야를 만들기 위해서 어떤 접근을 해야 할까? 강화학습을 이미 아는 상태에서 강화학습의 원로들이 했을 생각을 시뮬레이션해보자:

<em>현재는 우리가 원하는 바를 이뤄주는 인공지능을 만들기 위해 엄청난 양의 if-else를 사용한 의사결정 모델을 만들고 있다. 하지만 우리가 원하는 목표는 복잡하다. 예를 들어 축구를 하는 로봇에 탑재될 인공지능을 모터 제어 레벨에서부터 전술을 고려하는 지능까지 모두 if-else 문으로 만들 수 있을까? 만약 우리가 어떤 목표에 대한 보상만 잘 정의해 주고, 인공지능은 그 보상을 최대화 하는 방향으로 알아서 발전할 수 있다면? 이는 더욱 복잡한 인공지능을 만들 수 있는 현실적인 방안이 될 수 있다. 만약 인공지능 모델과 보상, 환경과의 상호작용을 수학적으로 정의할 수 있다면, 최적화 이론을 통해서 인공지능 모델로 하여금 환경과의 상호작용을 통해 인간의 개입 없이 보상을 최대화할 수 있도록 할 수 있지 않을까. **보상을 최대화하는 정책을 해로써 구할 수 있는 수학적인 모델이 필요하다.**
보상을 최대화 하는 정책을 $\pi$라고 하자.
$\pi$ 라는 함수의 공역과 치역은 무엇일까? 공역은 감각정보가 될 것이고, 치역은 아웃풋(액션)이 될 것이다. 액션은 두 가지를 야기하는데, 첫번째로 액션은 상태의 변화에 영향을 미치게 된다. 그리고 우리가 원하는 변화를 이뤄냈다면 보상이 이뤄져야 한다. 즉, 액션 이후로 변한 상태와 보상이 정책에게 던져져야 한다...</em>

보상을 통해 원하는 목표를 이뤄줄 수 있는 정책을 만들고 싶을 떄, 강화학습 이전에는 if-else를 사용한 노가다 코딩을 하고 있었다면, 어떤 똑똑하신 분들이 의사결정 과정을 수학적으로 모델링하려는 시도를 하신 것이다. 그것이 마르코브 결정 과정(Markov decision process, MDP)이다. 그럼 MDP의 개발 배경은(논문에 언급된게 아닌, 저자가 생각한 개발 배경이다) 다음의 목적을 가지고 있었다고 생각할 수 있다:

**보상을 최대화하는 정책을 해로써 구할 수 있는 수학적인 모델이 필요하다.**

이러한 생각을 가지고 MDP의 정의를 살펴보았다.

# 유한 마르코브 결정 과정(finite MDP)
## 정의(Definition)
**MDP**:
액션을 주면 다음 상태와 보상을 반환하는 환경 아래에서, 보상을 최대화하고자 하는 연속적 의사결정에 대한 모델

여기서 보상을 최대화 한다는 것은 보상 가설에 기반하며, 고로 'MDP는 목표를 달성하기 위한 연속적 의사결정에 대한 수학적 모델링인데, 액션을 주면 다음 상태와 보상을 반환하는 환경을 가정한 모델' 로 정의할 수 있다.

![MDP](/assets/img/post/2024-09-08/mdp.png)_MDP 도표[^2]_

### 변수

- 시간 $t$에서 환경의 상태(state) : $ S_{t} \in \mathcal{S} $
- 다음 시간 $t+1$에서 받게될 보상(reward) : $ R_{t+1} \in \mathcal{R} \subset \mathbb{R} $
- 행동(action): $A_t \in \mathcal{A}(s)$

정책과 환경이 상호작용하면 아래와 같은 궤적(trajectory)가 생성된다:

$$ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, S_3, ...$$

### 유한 MDP
유한 MDP는 $\mathcal{S}$, $\mathcal{A}$, $\mathcal{R}$ 의 집합이 유한한 크기를 가졌을 때를 뜻한다.

### 정책(Policy)
정책은 상태 $s \in \mathcal{S}$ 를 에서 가능한 액션 $a \in \mathcal{A}(s)$ 의 확률을 반환하는 함수이다. 결과적으로 우리가 최적값을 구하고 싶은 함수이다.

$$ \pi(a|s) \doteq Pr\{A_t=a | S_t=s\} $$

### 환경(동역학)

MDP에서 환경은 곧 동역학(dynamics) $p$ 로 정의된다:

$$ p(s^{\prime}, r | s, a) \doteq Pr\{S_t=s^{\prime}, R_t=r | S_{t-1}=s, A_{t-1}=a\} $$

환경은 $t$시간에서의 상태 $s_t$와, $s_t$에서 액션 $a$를 기반으로 다음상태 $s_{t+1}$에서 보상 $r$이 나올 확률을 반환하는 함수이며, 아래를 만족한다:

$$ \forall{s \in S}, \forall{a \in A(s)} , \sum_{s^{\prime}\in S}\sum_{r \in \mathcal{R}}p(s^{\prime}, r | s, a) = 1$$



## 정의의 확장
위에서는 정의를 살펴보았고, 이제부터는 위 정의로부터 확장되어, 이후 MDP에 기반한 강화학습을 디자인하는데 필요한 수식들을 살펴보자.
### 동역학의 확장
동역학 $p$의 정의는 아래와 같다:

$$ p(s^{\prime}, r | s, a) \doteq Pr\{S_t=s^{\prime}, R_t=r | S_{t-1}=s, A_{t-1}=a\} $$

여기서 우리는 **상태 전이 확룔(state-transition probability)**과 **보상 함수(reward function)**을 계산할 수 있다.

상태 전이 확률:

$$p(s^{\prime} | s, a) \doteq Pr\{S_t = s^{\prime} | S_{t-1}=s, A_{t-1} = a \} = \sum_{r \in \mathcal{R}}p(s^{\prime}, r | s, a) $$

보상 함수:

$$ r(s, a) \doteq \mathbb{E}[\mathcal{R}_t | S_{t-1} = s, A_{t-1} = a]  = \sum_{r \in \mathcal{R}} r \sum_{s^{\prime} \in \mathcal{S}}p(s^{\prime}, r | s, a)$$

### 보상의 확장
보상은 매 시간간격마다 나온다. 정책 목표를 더 명확히 하면: 정책이 받게 되는 누적 보상을 최대화해야 한다. 이를 수학적으로 정의할 떄, 가장 간단한 방법은 아래와 같이 모든 보상을 더하는 것이다:

$$ G_t \doteq R_{t+1} + R_{t+2} + R_{t+1} + ... + R_T $$

이때 $T$는 에피소드의 마지막 시간을 나타낸다.

유한한 시간 내의 일을 에피소딕 작업(episodic task)이라고 한다. 반면 연속적인 작업(continuing task)은 별도의 종단상태가 없는 경우를 뜻하는데, 이 경우 단순히 보상의 합을 하게 되면 최대화하려고 하는 보상 자체가 무한이 될 수 있기 때문에, 할인(discounting)의 개념을 추가하여 연속적인 작업에서도 누적 보상이 수렴하게끔 만들 수 있다:

$$ G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+1} + ... = \sum^{\infty}_{k=0}\gamma^{k}R_{t+k+1} $$

여기서 $\gamma$는 할인율(discount rate)라고 불리며, $ 0 \leq \gamma \leq 1$의 범위를 갖는다[^2].

> 나만의 생각: reward를 매 순간이 아닌 terminal에만 주도록 설계하면, discount factor와 무한발산 등을 고려하지 않은 채로 더욱 단순한 mdp 설계가 가능할까?
{: .prompt-info }


# MDP의 특징
MDP에서 생각해볼 만한 것들을 따로 정리해 보았다.
## 동역학
동역학 $p$에 따르면 $s^{\prime}$과 $r$ 는 전적으로 이전상태 $s$와 $a$에 따라 결정되는데, 이러한 조건을 만족하는 상태를 **마르코프 특성(Markov property)**를 가졌다고 일컬어진다[^2]. MDP는 환경에 있어서 왜 이렇게 강력한 제약 사항을 걸어 둔 것일까? 환경 정의의 보편성 보다는 최적 정책을 구할 때의 용이성 때문에 만든 현실적인 이유일 것 같다.



## References
[^1]: Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section. Wiley.
[^2]: Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed., p. 58). MIT Press.
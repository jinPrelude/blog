---
title: '[다시보는 기초 강화학습] 2. 정책과 가치 함수'
date: 2024-09-09 00:17:00 +0900
categories: ['다시보는 기초 강화학습']
tags: [rl] # TAG names should always be lowercase
author: euijin_jeong
math: true
toc: true
---
> Note: 이 글은 책 Reinforcement Learning: an Introduction 을 보고 제 생각과 함께 정리한 내용입니다.

본 글에서는 **가치함수(value function)**, **행동 가치 함수(action-value function)**, **최적 가치함수(optimal value function)**, **최적 행동 가치 함수(optimal action-value function)**을 알아 볼 것이다. 위 함수 모두 MDP에 기반한 강화학습을 디자인할 때의 필요성에 의해 정의된다.

# 정책(Policy)
이전 글에서 정책을 아래와 같이 정의했었다:

$$ \pi(a|s) \doteq Pr\{A_t=a | S_t=s\} $$


# 최적정책과 가치 함수

## 최적 정책
우리는 누적 보상을 최대화하는 정책 $\pi$를 찾아야 한다. 우리는 아직 알지 못하지만 어떠한 MDP 환경의 누적 보상을 최대화 하는 정책인 **최적 정책(optimal policy)**이 존재하며, 이를 $ \pi_* $라고 했을 때, 강화학습의 목표는 이 미지의 최적정책 $ \pi_* $를 찾아내는 것이다. 이러한 목표는 아래와 같이 표현된다:

$$ \pi_* = \arg\max_{\pi}\mathbb{E}_{\pi}[G_t|S_t=s], \forall s \in S $$

최적 정책은 우리가 강화학습을 통해 최종적으로 구하고자 하는 값이다.

## 가치 함수
위 최적 정책 식의 우변을 자세히 살펴보자.
우변은 상태 $s$에서 시작했을 때, 누적 보상의 기댓값을 최대화하는 정책 $\pi$를 뜻한다.
여기서 우변에서 $\arg\max$를 빼게 되면: <em>"임의의 정책 $\pi$가 임의의 상태 $s$에 있을때, 상태 $s$ 이후로 $\pi$가 얻을 것으로 기대되는 누적보상의 기댓값"</em>이라는 의미가 되며, 이를 우리는 **정책 $\pi$에 대한 가치함수(value function for policy $\pi$)**이라고 부른다. 앞으로 줄여서 **가치함수(value function)**라고 부르게 되지만, 가치함수는 임의의 정책에 대한 함수라는 점을 잊지 말자.

$$ v_{\pi}(s) \doteq \mathbb{E}_{\pi}[G_t|S_t=s] = \mathbb{E}_{\pi}[\sum^{\infty}_{k=0}\gamma^{k}R_{t+k+1}|S_t=s], \forall s \in S $$

가치함수의 식에서 우리는 2가지를 알 수 있다:
1. 가치함수는 임의의 정책 $\pi$에 종속적인 함수이다.
2. 가치함수는 모든 상태 $ s \in S $에서 정책 $\pi$가 얻을 것으로 기대되는 누적 보상의 기댓값만 계산해서 알려줄 뿐이다.

1번과 같이 가치함수는 정책 $\pi$에 종속적인 함수여서, $\pi$가 변하면 가치함수도 변한다.

그리고 2번과 같이 가치함수는 상태 $s$에서 정책 $\pi$가 받게될 누적보상의 기댓값은 알려주지만, 어떻게 행동해야 할 지에 대한 지침은 전혀 알려주지 않는다(예를 들어 <em>"가치 함수를 통해 현재 상태에서 가능한 다음 상태들의 가치함수 기댓값을 모두 살펴본 후, 가치함수의 기댓값이 가장 큰 다음 상태로 옮겨가게끔 action을 취하면 되지 않나요?"</em>라고 할 수 있지만, 그러한 행동(greedy action selection) 또한 정책을 기반으로 판단하는 또다른 판단 주체라고 본다).


